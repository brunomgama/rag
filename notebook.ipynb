{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:21.388528Z",
     "start_time": "2025-01-25T10:30:21.382692Z"
    }
   },
   "source": [
    "# pip install -qU langchain-text-splitters\n",
    "# pip install qdrant-client\n",
    "# pip install python-dotenv\n",
    "# pip install sentence-transformers\n",
    "# pip install qdrant-client\n",
    "# pip install -qU langchain_community pypdf\n",
    "\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from litellm import completion\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "COLLECTION_NAME = \"qa_index\"\n",
    "\n",
    "LLM_MODEL = \"ollama/llama3.2\"\n",
    "API_BASE = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "Context: {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Question: Based on the provided documents {question} If the information is not provided on the documents then answer I don't know.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "QUESTION = \"Which are the 12 Factors on the 12 Factor App Framework?\"\n"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:21.614071Z",
     "start_time": "2025-01-25T10:30:21.392758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load all PDFs from the 'data' folder\n",
    "loader = DirectoryLoader(\n",
    "    path=DATA_PATH,\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "text_chunks = text_splitter.split_documents(documents)"
   ],
   "id": "950437a2c2dee97",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:24.874365Z",
     "start_time": "2025-01-25T10:30:21.615319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 4. Initialize the embedding model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "\n",
    "# 5. Encode the page_content from each chunk\n",
    "#    (rather than passing the entire Document object)\n",
    "string_chunks = [chunk.page_content for chunk in text_chunks]\n",
    "embeddings = embedding_model.encode(string_chunks, show_progress_bar=True)\n",
    "\n",
    "# 6. Inspect the shape of the first embedding\n",
    "print(\"Shape of the first embedding vector:\", embeddings[0].shape)"
   ],
   "id": "8997123fed20e61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e07d59e32ea94207bee5eecacf50ecd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first embedding vector: (384,)\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant"
   ],
   "id": "f62b14f3f804059a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.153790Z",
     "start_time": "2025-01-25T10:30:24.875419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Upsert to Qdrant\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")"
   ],
   "id": "ddfc562c3d58f03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.158056Z",
     "start_time": "2025-01-25T10:30:25.155423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = []\n",
    "payload = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    ids.append(i)\n",
    "    payload.append({\n",
    "        \"source\": chunk.metadata,\n",
    "        \"content\": chunk.page_content\n",
    "    })"
   ],
   "id": "8d3fe9075bef332c",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.184202Z",
     "start_time": "2025-01-25T10:30:25.159196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.upload_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    "    ids=ids,\n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "client.count(COLLECTION_NAME)"
   ],
   "id": "fd1b5ba258d9094d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=40)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.188485Z",
     "start_time": "2025-01-25T10:30:25.185486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(text: str, top_k: int):\n",
    "    query_embedding = embedding_model.encode(text).tolist()\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=None,\n",
    "        limit=top_k\n",
    "    )\n",
    "    return search_result"
   ],
   "id": "12dc5bbbd36b60f",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.225246Z",
     "start_time": "2025-01-25T10:30:25.189249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = search(QUESTION, top_k=5)\n",
    "#results"
   ],
   "id": "a033d0d91b932f97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/sf857rrx2pnckk7268njzx940000gn/T/ipykernel_77849/566791645.py:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:25.228525Z",
     "start_time": "2025-01-25T10:30:25.226200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the actual strings from Qdrant\n",
    "references = [obj.payload[\"content\"] for obj in results]\n",
    "\n",
    "# 'references' is now a list of strings\n",
    "context = \"\\n\\n\".join(references)"
   ],
   "id": "46b8751cb4fbb80",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T10:30:32.145637Z",
     "start_time": "2025-01-25T10:30:25.229650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = completion(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{\"content\": SYSTEM_PROMPT.format(context=context),\"role\": \"system\"}, {\"content\": USER_PROMPT.format(question=QUESTION),\"role\": \"user\"}],\n",
    "    api_base=API_BASE,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "id": "a8ab566f65a4c59d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12 factors of the 12-Factor App framework, as described in the provided documentation, are:\n",
      "\n",
      "1. Codebase\n",
      "2. Dependencies\n",
      "3. Config\n",
      "4. Backing Services\n",
      "5. Build, Release, Run\n",
      "6. Processes\n",
      "7. Port Binding\n",
      "8. Concurrency\n",
      "9. Disposability\n",
      "10. Dev/Prod Parity\n",
      "11. Logs\n",
      "12. Admin Processes\n",
      "\n",
      "Note that the list provided in the document is not exactly as mentioned in your query but, it seems that this is what was provided by you"
     ]
    }
   ],
   "execution_count": 161
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
